{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Loading the model\n",
    "\n",
    "For this practice, we will be using GloVe word vectors converted into word2vec format.\n",
    "\n",
    "The \"50d\" indicates that our words are represented as 50 dimensional vectors.\n",
    "\n",
    "This was chosen to keep the file size small and inference fast, but if you are willing to see how bigger dimensional vectors work then you can download the GloVe vectors at [this zip file](https://nlp.stanford.edu/data/glove.6B.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = datapath(f'{directory}/glove.6B.50d.txt') # Original glove file here\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.50d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Word relationship and similarities\n",
    "\n",
    "The \"most_similar\" function lets you see the well... \"most similar\" word of your input.\n",
    "\n",
    "It can also take \"negative\" as an argument and return the furthest word from your input. \n",
    "\n",
    "Give various words a try by assigning a word to the \"your_word\" variable!\n",
    "\n",
    "Also give the \"analogy\" function a try. (Guess what this function does!)\n",
    "\n",
    "And input your own set of words by in the cell marked as \"your example\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the word that you want to try out (be creative!)\n",
    "your_word = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(your_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(negative=your_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(x1, x2, y1):\n",
    "    result = model.most_similar(positive=[y1, x2], negative=[x1])\n",
    "    return result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 1\n",
    "analogy('japan', 'japanese', 'australia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 2\n",
    "analogy('king', 'man', 'queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your example\n",
    "analogy(, , )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Visualizing the word vector space\n",
    "\n",
    "The \"display_pca_scatterplot\" function takes a list of words as input and visualizes the vector space of those words for you.\n",
    "\n",
    "Try to fill the list \"your_words\" with your own choice of words (at least 20) and see how they are represented in the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_pca_scatterplot(model, words=None, sample=0, filename=\"gensim.png\"):\n",
    "    if words == None:\n",
    "        if sample > 0:\n",
    "            words = np.random.choice(list(model.vocab.keys()), sample)\n",
    "        else:\n",
    "            words = [ word for word in model.vocab ]\n",
    "        \n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "\n",
    "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)\n",
    "    \n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_pca_scatterplot(model, \n",
    "                        [\"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n",
    "                        \"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"dumb\",\n",
    "                        \"annoying\", \"female\", \"male\", \"queen\", \"king\", \"man\", \"woman\", \"rain\", \"snow\",\n",
    "                        \"hail\", \"coffee\", \"tea\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your own set of words to see how they are represented\n",
    "your_words = []\n",
    "\n",
    "display_pca_scatterplot(model, your_words, filename=\"yourwords.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HW1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
